{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# ! pip install en_core_web_sm\n",
    "# ! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from spacy import displacy\n",
    "from spacy.attrs import LOWER \n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher\n",
    "import numpy as np\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframes from csv\n",
    "df = pd.read_csv('./data/Data/GeoCoding _transcribe_radio/transcribed_radio.csv')\n",
    "df_context = pd.read_csv('./data/Data/GeoCoding_transcribed_radio_with_street_context/transcribed_radio_with_street_context.csv')\n",
    "with open(\"./data/Pickles/streets.pkl\", \"rb\") as fp:\n",
    "    streets = pickle.load(fp)\n",
    "list_of_roads = list(streets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>transcripts</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>powers of</td>\n",
       "      <td>0.703373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Brevard Africa for 1636 thank you</td>\n",
       "      <td>0.789880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>is going to be</td>\n",
       "      <td>0.625621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ester Drive robson's plane set the driver knoc...</td>\n",
       "      <td>0.785905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Marcus 11:30 Highway Street 1 1 3 0 hi</td>\n",
       "      <td>0.805028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        transcripts  confidence\n",
       "0           0                                          powers of    0.703373\n",
       "1           1                  Brevard Africa for 1636 thank you    0.789880\n",
       "2           2                                     is going to be    0.625621\n",
       "3           3  Ester Drive robson's plane set the driver knoc...    0.785905\n",
       "4           4             Marcus 11:30 Highway Street 1 1 3 0 hi    0.805028"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first five rows\n",
    "df_context.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spaCy](https://spacy.io/) finds entities in a document by tokenizing the strings and then assigning each word a tag. It then looks for patterns to get entities and classifies them with labels. Here we used spaCy to extract entities with labels related to locations (GPE and FAC) to be able to extract them from our transcripts. GPE is the acronym for \"geo-political entities\" and FAC, \"facility\", which locates airports, highways, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract locations using spaCy pre trained labels\n",
    "def location_extraction(string_in):\n",
    "    doc = nlp(string_in)\n",
    "    locations = []\n",
    "    # loop through every entity in the transcript\n",
    "    for X in doc.ents:\n",
    "        if (X.label_ == 'FAC') or (X.label_ == 'GPE'):\n",
    "            locations.append(X.text)\n",
    "    if len(locations) != 0:\n",
    "        return locations\n",
    "    return None\n",
    "\n",
    "# Add a column with the extracted locations\n",
    "df['location_extraction'] = df['transcripts'].map(location_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  spaCy Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use spaCy Matcher entity to be able to generate our own set of rules to look for in the text. Every rule corresponds to patterns which consists of sets of words, conditions and operators, where the word had to be found in the document following a specific condition and the operator determines how many times or how we have to observe the pattern.\n",
    "Here we are looking for entities that correspond to a road name in Butte county, and since we already have a complete list of road names, we can set one rule for each road, where the pattern would specify and all words have to match exactly one time except if the name ended with a generic word like \"street\" or \"Road\" then that word could match 0 or more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Matcher entity\n",
    "\n",
    "# Instantiate\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# specifies what spacy does when it finds a match in the document. Here we just want to return the matches\n",
    "def on_match(matcher, doc, id, matches):\n",
    "    return matches\n",
    "\n",
    "# building patterns for every road name, the condition being that the lowercase entity in the doc should match \n",
    "# the lowercase verion of the road name, so that capitalization wouldn't affect the model\n",
    "def build_pattern(road_name):\n",
    "    list_words = road_name.split(' ')\n",
    "    # general words that appear a lot in the list. \n",
    "    # The reason why we do this is to still get a match if they are not present\n",
    "    roads_general = ['lane', 'road', \n",
    "                 'court', 'drive', \n",
    "                 'avenue', 'way', \n",
    "                 'street', 'circle', \n",
    "                 'place', 'highway', 'trail']\n",
    "    if list_words[-1].lower() in roads_general:\n",
    "        pattern = [{'LOWER': word.lower()} for word in list_words[:-1]]\n",
    "        pattern.append({'op': '*', 'LOWER' : list_words[-1].lower()})\n",
    "    else:\n",
    "        pattern = [{'LOWER': word.lower()} for word in list_words]\n",
    "    return pattern\n",
    "\n",
    "# Get a pattern of every road\n",
    "for road in list_of_roads:\n",
    "    matcher.add(road, on_match, build_pattern(road))\n",
    "    \n",
    "# This function takes a string as input and returns it with every word capitalized\n",
    "def capitalize_string(string_in):\n",
    "    words = string_in.split(' ')\n",
    "    string_out = ''\n",
    "    for i in words:\n",
    "        string_out += i.capitalize() + ' '\n",
    "    string_out = string_out[:-1]\n",
    "    return string_out   \n",
    "    \n",
    "# Look for locations in the transcript, then extract them\n",
    "def location_extraction_context(string_in):\n",
    "    doc = nlp(string_in)\n",
    "    string_out = ''\n",
    "    list_words = string_in.split(' ')\n",
    "    matches = matcher(doc)\n",
    "    if len(matches) == 0:\n",
    "        return None\n",
    "    indeces_to_pop = []\n",
    "    # loop through the matches and delete those that are a subset of another. \n",
    "    # this was done because some road names have words in commond and we were getting 2 matches for some locations\n",
    "    # here we eliminate the shorter one since the longest is clearly the one intended\n",
    "    for a in range(len(matches)):\n",
    "        for b in range(a+1, len(matches)):\n",
    "            if (matches[a][2] == matches[b][2]):\n",
    "                if (matches[a][1] < matches[b][1]):\n",
    "                    indeces_to_pop.append(b)\n",
    "                else:\n",
    "                    indeces_to_pop.append(a)\n",
    "    matches_final = [tup for index, tup in enumerate(matches) if index not in indeces_to_pop]\n",
    "    # loop through the matches and add them to the string to return\n",
    "    # matches consist of an id and the indeces of the first and last word that constitute the pattern in the document\n",
    "    # we use the ids to extrat the locations from the rules in the Matcher instance and not from the text itself,\n",
    "    # to make sure they all follow the same format\n",
    "    for match in matches_final:\n",
    "        list_pattern = matcher.get(match[0])[1][0]\n",
    "        for token in list_pattern:\n",
    "            string_out += token['LOWER'] + ' '\n",
    "        string_out += ', '\n",
    "    string_out = string_out[:-3]\n",
    "    string_out = capitalize_string(string_out)\n",
    "    return string_out\n",
    "\n",
    "# Add a column with the extracted locations\n",
    "df_context['location_extraction'] = df_context['transcripts'].map(location_extraction_context)\n",
    "\n",
    "# Since we dont care about transcripts where we didn't find any locations we drop all NAs\n",
    "df_context.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "4    None\n",
       "Name: location_extraction, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['location_extraction'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filename'] = np.round(df['confidence'], 6).astype(str)\n",
    "df_context['filename'] = np.round(df_context['confidence'], 6).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>transcripts</th>\n",
       "      <th>confidence</th>\n",
       "      <th>location_extraction</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Peridot we close races here</td>\n",
       "      <td>0.825849</td>\n",
       "      <td>Peridot Place</td>\n",
       "      <td>0.825849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Library 612 to myself Street between 6th Avenu...</td>\n",
       "      <td>0.788517</td>\n",
       "      <td>6th Street , Julia Street</td>\n",
       "      <td>0.788517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        transcripts  confidence  \\\n",
       "6           6                        Peridot we close races here    0.825849   \n",
       "7           7  Library 612 to myself Street between 6th Avenu...    0.788517   \n",
       "\n",
       "         location_extraction  filename  \n",
       "6              Peridot Place  0.825849  \n",
       "7  6th Street , Julia Street  0.788517  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_context.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>transcripts</th>\n",
       "      <th>confidence</th>\n",
       "      <th>location_extraction</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I was just</td>\n",
       "      <td>0.502997</td>\n",
       "      <td>None</td>\n",
       "      <td>0.502997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>our brothers are critical for 1636 thank you</td>\n",
       "      <td>0.842904</td>\n",
       "      <td>None</td>\n",
       "      <td>0.842904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>is going to be</td>\n",
       "      <td>0.652573</td>\n",
       "      <td>None</td>\n",
       "      <td>0.652573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>to drive robson's plane set the driver knocked...</td>\n",
       "      <td>0.700766</td>\n",
       "      <td>None</td>\n",
       "      <td>0.700766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Market 11:30 high was 31130 hi</td>\n",
       "      <td>0.803496</td>\n",
       "      <td>None</td>\n",
       "      <td>0.803496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        transcripts  confidence  \\\n",
       "0           0                                         I was just    0.502997   \n",
       "1           1       our brothers are critical for 1636 thank you    0.842904   \n",
       "2           2                                     is going to be    0.652573   \n",
       "3           3  to drive robson's plane set the driver knocked...    0.700766   \n",
       "4           4                     Market 11:30 high was 31130 hi    0.803496   \n",
       "\n",
       "  location_extraction  filename  \n",
       "0                None  0.502997  \n",
       "1                None  0.842904  \n",
       "2                None  0.652573  \n",
       "3                None  0.700766  \n",
       "4                None  0.803496  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save them as csv\n",
    "df.to_csv(f\"./data/Data/Long_and_Lat_result /{df['filename']}.csv\")\n",
    "df_context.to_csv(f\"./data/Data/Long_and_Lat_result_w_o_context/{df_context['filename']}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
